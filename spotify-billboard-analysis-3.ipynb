{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# git config --global credential.helper store\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import ssl\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                            GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"stopwords\", \"punkt\", \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"])\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from clean_dfs import clean_features, clean_weeks, clean_lyrics\n",
    "from web_scraping import parse_page, store_lyrics, filter_profanity\n",
    "from nlp_pipeline import lyrics_tokenize, get_tfidf_matrix\n",
    "from genre_helper_functions import get_bucket, contains_genre_type, create_genre_column\n",
    "import make_plots as plots\n",
    "import modeling_functions as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_features()\n",
    "#weeks = clean_weeks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joined = weeks.merge(features, on=\\'SongID\\')\\n#joined.to_csv(\"data/joined.csv\", index=False)\\n\\n# Expand genres into individual components\\nfeatureGenres = features.explode(\\'spotify_genre\\')\\nfeatureGenres = featureGenres[featureGenres[\\'spotify_genre\\'] != \\'\\']\\n\\njoinedGenres = joined.explode(\\'spotify_genre\\')\\njoinedGenres = joinedGenres[joinedGenres[\\'spotify_genre\\'] != \\'\\']\\n\\nexplicitness = joined[[\\'Year\\', \\'spotify_track_explicit\\']]\\nexplicitness = explicitness.groupby([\\'Year\\']).mean().reset_index()\\n\\nnumericalMetrics = joined.columns.tolist()[11:23]\\nnumericals = joined[[\\'Year\\'] + numericalMetrics].groupby([\\'Year\\']).mean().reset_index()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''joined = weeks.merge(features, on='SongID')\n",
    "#joined.to_csv(\"data/joined.csv\", index=False)\n",
    "\n",
    "# Expand genres into individual components\n",
    "featureGenres = features.explode('spotify_genre')\n",
    "featureGenres = featureGenres[featureGenres['spotify_genre'] != '']\n",
    "\n",
    "joinedGenres = joined.explode('spotify_genre')\n",
    "joinedGenres = joinedGenres[joinedGenres['spotify_genre'] != '']\n",
    "\n",
    "explicitness = joined[['Year', 'spotify_track_explicit']]\n",
    "explicitness = explicitness.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "numericalMetrics = joined.columns.tolist()[11:23]\n",
    "numericals = joined[['Year'] + numericalMetrics].groupby(['Year']).mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Normalize numerical features not between 0 and 1\\nfeatureGenresNorm = featureGenres.copy()\\nscaled = [\"track_duration\", \"loudness\", \"tempo\"]\\nfor metric in scaled:\\n    mms = MinMaxScaler()\\n    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm[\\'track_duration\\'].                                 to_numpy().reshape(-1, 1))\\n\\n# Create grouped tables\\ngenres = featureGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoined = joinedGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoinedDecade = joinedGenres.groupby([\\'spotify_genre\\', \\'Decade\\'])[\\'SongID\\'].count().                         reset_index().sort_values(by=\"Decade\")\\ngenreFeatures = featureGenresNorm.groupby([\\'spotify_genre\\'])[numericalMetrics].mean().reset_index()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Normalize numerical features not between 0 and 1\n",
    "featureGenresNorm = featureGenres.copy()\n",
    "scaled = [\"track_duration\", \"loudness\", \"tempo\"]\n",
    "for metric in scaled:\n",
    "    mms = MinMaxScaler()\n",
    "    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm['track_duration']. \\\n",
    "                                to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Create grouped tables\n",
    "genres = featureGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoined = joinedGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoinedDecade = joinedGenres.groupby(['spotify_genre', 'Decade'])['SongID'].count(). \\\n",
    "                        reset_index().sort_values(by=\"Decade\")\n",
    "genreFeatures = featureGenresNorm.groupby(['spotify_genre'])[numericalMetrics].mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Web scrape lyrics\\nfeatureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre                              in features[\\'spotify_genre\\']]].reset_index(drop=True)\\nlyricsMap = {}\\nthreads = []\\ntemp = 0\\nstart = time.time()\\n# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\\n#for i in range(temp, temp+50):\\nfor i in range(len(featureScrape)):\\n    t = Thread(target=store_lyrics, args=(featureScrape[\\'Song\\'][i],\\n                featureScrape[\\'Performer\\'][i], lyricsMap))\\n    threads.append(t)\\n    t.start()\\nfor t in threads:\\n    t.join()\\nend = time.time()\\nprint(end - start)\\nscrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\\nscrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\\n\\n# Get list of all improperly formatted songs and save to file\\nproblemSongs = []\\nfor k, v in lyricsMap.items():\\n    if v[0][0] == \"*\":\\n        problemSongs.append([k] + v[2:5])\\nprint(len(featureScrape), len(problemSongs))\\n\\nwith open(\"data/problemSongs.txt\", \"w\") as file:\\n    for s in problemSongs:\\n        file.write(\"{}\\n\".format(s))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Web scrape lyrics\n",
    "featureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre \\\n",
    "                             in features['spotify_genre']]].reset_index(drop=True)\n",
    "lyricsMap = {}\n",
    "threads = []\n",
    "temp = 0\n",
    "start = time.time()\n",
    "# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\n",
    "#for i in range(temp, temp+50):\n",
    "for i in range(len(featureScrape)):\n",
    "    t = Thread(target=store_lyrics, args=(featureScrape['Song'][i],\n",
    "                featureScrape['Performer'][i], lyricsMap))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "scrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\n",
    "scrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\n",
    "\n",
    "# Get list of all improperly formatted songs and save to file\n",
    "problemSongs = []\n",
    "for k, v in lyricsMap.items():\n",
    "    if v[0][0] == \"*\":\n",
    "        problemSongs.append([k] + v[2:5])\n",
    "print(len(featureScrape), len(problemSongs))\n",
    "\n",
    "with open(\"data/problemSongs.txt\", \"w\") as file:\n",
    "    for s in problemSongs:\n",
    "        file.write(\"{}\\n\".format(s))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv of previously outputted scraped lyrics and reformat to match original\n",
    "# allLyrics = clean_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NLP pipeline to create tokens from lyrics\\nallLyrics[\\'Lyrics_tokenized\\'] = list(map(lyrics_tokenize, allLyrics[\\'Lyrics\\']))\\nallLyrics.dropna(inplace=True)\\nallLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# NLP pipeline to create tokens from lyrics\n",
    "allLyrics['Lyrics_tokenized'] = list(map(lyrics_tokenize, allLyrics['Lyrics']))\n",
    "allLyrics.dropna(inplace=True)\n",
    "allLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus and make dataframe with TF-IDF matrix\n",
    "allLyrics = pd.read_csv(\"data/lyricsTokenized.csv\")\n",
    "allLyrics.dropna(inplace=True)\n",
    "corpus = allLyrics['Lyrics_tokenized']\n",
    "tfidfLyrics = get_tfidf_matrix(corpus, 5000)\n",
    "tfidfLyrics.insert(0, \"SongID\", allLyrics['SongID'])\n",
    "tfidfLyrics.to_csv(\"data/tfidfMatrix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models with only lyrics, not counting other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with valence column from features to get valence of each song\n",
    "valenceOnly = pd.DataFrame({\"SongID\": features['SongID'], \"spotify_genre\": \n",
    "                            features['spotify_genre'], \"valence\": features['valence']})\n",
    "lyricsAndValence = tfidfLyrics.merge(valenceOnly, on='SongID')\n",
    "lyricsAndValence.set_index(\"SongID\", inplace=True)\n",
    "# Create new dataframe using classifier instead of regressor\n",
    "lyricsAndValenceBin = lyricsAndValence.copy()\n",
    "lyricsAndValenceBin['valence'] = (lyricsAndValenceBin['valence'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Run classifier models for pop genre\n",
    "lyricsAndValenceBinPop = lyricsAndValenceBin[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                                for g in lyricsAndValenceBin['spotify_genre']]]\n",
    "lyricsAndValenceBinPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceBinPop[lyricsAndValenceBinPop.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceBinPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5558646267964766, 0.39567430025445294, 0.3916876574307305)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.5559, 0.3957, 0.3917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time.time()\\nmodel.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\\nend = time.time()\\nprint(end-start)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3,                                         X_train, X_test, y_train, y_test)\\nprint(gradient_boost_class_results)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.6347, 0.0585, 0.4894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Run regressor models for pop genre\n",
    "lyricsAndValencePop = lyricsAndValence[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                            for g in lyricsAndValence['spotify_genre']]]\n",
    "lyricsAndValencePop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValencePop[lyricsAndValencePop.columns.difference(['valence'])]\n",
    "y = lyricsAndValencePop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\\ngbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\\nprint(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\\nscoreValencePop = gbr.score(X_test, y_test)\\ny_pred = gbr.predict(X_test)\\nrmseValencePop = np.sqrt(mean_squared_error(y_test, y_pred))\\nprint(scoreValencePop, rmseValencePop)\\ngradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3,                                         X_train, X_test, y_train, y_test)\\nprint(gradient_boost_class_results)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreValencePop = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseValencePop = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreValencePop, rmseValencePop)\n",
    "gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time.time()\\nmodel.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\\nend = time.time()\\nprint(end-start)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1f1b2e2eafa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Gradient boosting regressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgradient_boost_reg_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_boost_reg_results\u001b[0m\u001b[0;34m(\u001b[0m                             \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_boost_reg_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 0.006539, 0.2334\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/spotify-billboard-analysis/modeling_functions.py\u001b[0m in \u001b[0;36mget_gradient_boost_reg_results\u001b[0;34m(learning_rate, num_trees, max_depth, xtrain, xtest, ytrain, ytest)\u001b[0m\n\u001b[1;32m    241\u001b[0m                         np.array) -> ((float, float), np.array):\n\u001b[1;32m    242\u001b[0m     gbc = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=num_trees, \\\n\u001b[0;32m--> 243\u001b[0;31m                                     subsample=0.5, max_depth=max_depth).fit(xtrain, ytrain)\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1608\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1609\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1244\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.006539, 0.2334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "filteredWords = np.array(filter_profanity(lyricsAndValencePop.columns[:-1]))\n",
    "plots.make_feature_importance_plot(feature_importances, filteredWords, 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Pop (valence only)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_valencepop.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'modeling_functions' has no attribute 'get_mlp_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4f3b87c884db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Multilayer perceptron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mlp_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 0.2244\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'modeling_functions' has no attribute 'get_mlp_score'"
     ]
    }
   ],
   "source": [
    "# Multilayer perceptron\n",
    "score = model.get_mlp_score(10, X_train, y_train, X_test, y_test)\n",
    "print(score)\n",
    "# 0.2244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for rock/metal genres\n",
    "lyricsAndValenceBinRock = lyricsAndValenceBin[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                                for g in lyricsAndValenceBin['spotify_genre']]]\n",
    "lyricsAndValenceBinRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceBinRock[lyricsAndValenceBinRock.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceBinRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.5877, 0.4042, 0.3867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.6644, 0.0436, 0.4630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for rock/metal genres\n",
    "lyricsAndValenceRock = lyricsAndValence[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                            for g in lyricsAndValence['spotify_genre']]]\n",
    "lyricsAndValenceRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceRock[lyricsAndValenceRock.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreValenceRock = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseValenceRock = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreValenceRock, rmseValenceRock)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.01272, 0.2339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "filteredWords = np.array(filter_profanity(lyricsAndValenceRock.columns[:-1]))\n",
    "plots.make_feature_importance_plot(feature_importances, filteredWords, 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Rock (valence only)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_valencerock.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "score = model.get_mlp_score(10, X_train, y_train, X_test, y_test)\n",
    "print(score)\n",
    "# 0.2082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lyricsAndValence, lyricsAndValencePop, lyricsAndValenceRock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try adding all other numerical features to see if it improves accuracy/RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with features to get all numerical features as well as valence\n",
    "lyricsAndFeatures = tfidfLyrics.merge(features, on='SongID')\n",
    "lyricsAndFeatures.drop([\"Performer\", \"Song\"], axis=1, inplace=True)\n",
    "lyricsAndFeatures.set_index(\"SongID\", inplace=True)\n",
    "# Create new dataframe using classifier instead of regressor\n",
    "lyricsAndFeaturesBin = lyricsAndFeatures.copy()\n",
    "lyricsAndFeaturesBin['valence'] = (lyricsAndFeaturesBin['valence'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for pop genre\n",
    "lyricsAndFeaturesBinPop = lyricsAndFeaturesBin[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                                for g in lyricsAndFeaturesBin['spotify_genre']]]\n",
    "lyricsAndFeaturesBinPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesBinPop[lyricsAndFeaturesBinPop.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesBinPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.6796, 0.5280, 0.5646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_dgradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.7821, 0.5954, 0.7548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for pop genre\n",
    "lyricsAndFeaturesPop = lyricsAndFeatures[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                            for g in lyricsAndFeatures['spotify_genre']]]\n",
    "lyricsAndFeaturesPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesPop[lyricsAndFeaturesPop.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreFeaturesPop = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseFeaturesPop = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreFeaturesPop, rmseFeaturesPop)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.4659, 0.1711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "filteredWords = np.array(filter_profanity(lyricsAndFeaturesPop.columns[:-1]))\n",
    "plots.make_feature_importance_plot(feature_importances, filteredWords, 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Pop (all features)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_featurespop.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "score = model.get_mlp_score(10, X_train, y_train, X_test, y_test)\n",
    "print(score)\n",
    "# 0.2244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for rock/metal genres\n",
    "lyricsAndFeaturesBinRock = lyricsAndFeaturesBin[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                                for g in lyricsAndFeaturesBin['spotify_genre']]]\n",
    "lyricsAndFeaturesBinRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesBinRock[lyricsAndFeaturesBinRock.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesBinRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.7294, 0.6028, 0.5925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.8055, 0.6045, 0.7626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for rock/metal genres\n",
    "lyricsAndFeaturesRock = lyricsAndFeatures[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                            for g in lyricsAndFeatures['spotify_genre']]]\n",
    "lyricsAndFeaturesRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesRock[lyricsAndFeaturesRock.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreFeaturesRock = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseFeaturesRock = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreFeaturesRock, rmseFeaturesRock)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.5324, 0.1610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "filteredWords = np.array(filter_profanity(lyricsAndFeaturesRock.columns[:-1]))\n",
    "plots.make_feature_importance_plot(feature_importances, filteredWords, 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Rock (all features)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_featuresrock.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "score = model.get_mlp_score(10, X_train, y_train, X_test, y_test)\n",
    "print(score)\n",
    "# 0.2082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lyricsAndFeatures, lyricsAndFeaturesPop, lyricsAndFeaturessRock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
