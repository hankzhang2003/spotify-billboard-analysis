{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# git config --global credential.helper store\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import ssl\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                            GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"stopwords\", \"punkt\", \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"])\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from clean_dfs import clean_features, clean_weeks, clean_lyrics\n",
    "from web_scraping import parse_page, store_lyrics, filter_profanity\n",
    "from nlp_pipeline import lyrics_tokenize, get_tfidf_matrix\n",
    "from genre_helper_functions import get_bucket, contains_genre_type, create_genre_column\n",
    "import make_plots as plots\n",
    "import modeling_functions as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_features()\n",
    "#weeks = clean_weeks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joined = weeks.merge(features, on=\\'SongID\\')\\n#joined.to_csv(\"data/joined.csv\", index=False)\\n\\n# Expand genres into individual components\\nfeatureGenres = features.explode(\\'spotify_genre\\')\\nfeatureGenres = featureGenres[featureGenres[\\'spotify_genre\\'] != \\'\\']\\n\\njoinedGenres = joined.explode(\\'spotify_genre\\')\\njoinedGenres = joinedGenres[joinedGenres[\\'spotify_genre\\'] != \\'\\']\\n\\nexplicitness = joined[[\\'Year\\', \\'spotify_track_explicit\\']]\\nexplicitness = explicitness.groupby([\\'Year\\']).mean().reset_index()\\n\\nnumericalMetrics = joined.columns.tolist()[11:23]\\nnumericals = joined[[\\'Year\\'] + numericalMetrics].groupby([\\'Year\\']).mean().reset_index()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''joined = weeks.merge(features, on='SongID')\n",
    "#joined.to_csv(\"data/joined.csv\", index=False)\n",
    "\n",
    "# Expand genres into individual components\n",
    "featureGenres = features.explode('spotify_genre')\n",
    "featureGenres = featureGenres[featureGenres['spotify_genre'] != '']\n",
    "\n",
    "joinedGenres = joined.explode('spotify_genre')\n",
    "joinedGenres = joinedGenres[joinedGenres['spotify_genre'] != '']\n",
    "\n",
    "explicitness = joined[['Year', 'spotify_track_explicit']]\n",
    "explicitness = explicitness.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "numericalMetrics = joined.columns.tolist()[11:23]\n",
    "numericals = joined[['Year'] + numericalMetrics].groupby(['Year']).mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Normalize numerical features not between 0 and 1\\nfeatureGenresNorm = featureGenres.copy()\\nscaled = [\"track_duration\", \"loudness\", \"tempo\"]\\nfor metric in scaled:\\n    mms = MinMaxScaler()\\n    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm[\\'track_duration\\'].                                 to_numpy().reshape(-1, 1))\\n\\n# Create grouped tables\\ngenres = featureGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoined = joinedGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoinedDecade = joinedGenres.groupby([\\'spotify_genre\\', \\'Decade\\'])[\\'SongID\\'].count().                         reset_index().sort_values(by=\"Decade\")\\ngenreFeatures = featureGenresNorm.groupby([\\'spotify_genre\\'])[numericalMetrics].mean().reset_index()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Normalize numerical features not between 0 and 1\n",
    "featureGenresNorm = featureGenres.copy()\n",
    "scaled = [\"track_duration\", \"loudness\", \"tempo\"]\n",
    "for metric in scaled:\n",
    "    mms = MinMaxScaler()\n",
    "    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm['track_duration']. \\\n",
    "                                to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Create grouped tables\n",
    "genres = featureGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoined = joinedGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoinedDecade = joinedGenres.groupby(['spotify_genre', 'Decade'])['SongID'].count(). \\\n",
    "                        reset_index().sort_values(by=\"Decade\")\n",
    "genreFeatures = featureGenresNorm.groupby(['spotify_genre'])[numericalMetrics].mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Web scrape lyrics\\nfeatureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre                              in features[\\'spotify_genre\\']]].reset_index(drop=True)\\nlyricsMap = {}\\nthreads = []\\ntemp = 0\\nstart = time.time()\\n# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\\n#for i in range(temp, temp+50):\\nfor i in range(len(featureScrape)):\\n    t = Thread(target=store_lyrics, args=(featureScrape[\\'Song\\'][i],\\n                featureScrape[\\'Performer\\'][i], lyricsMap))\\n    threads.append(t)\\n    t.start()\\nfor t in threads:\\n    t.join()\\nend = time.time()\\nprint(end - start)\\nscrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\\nscrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\\n\\n# Get list of all improperly formatted songs and save to file\\nproblemSongs = []\\nfor k, v in lyricsMap.items():\\n    if v[0][0] == \"*\":\\n        problemSongs.append([k] + v[2:5])\\nprint(len(featureScrape), len(problemSongs))\\n\\nwith open(\"data/problemSongs.txt\", \"w\") as file:\\n    for s in problemSongs:\\n        file.write(\"{}\\n\".format(s))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Web scrape lyrics\n",
    "featureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre \\\n",
    "                             in features['spotify_genre']]].reset_index(drop=True)\n",
    "lyricsMap = {}\n",
    "threads = []\n",
    "temp = 0\n",
    "start = time.time()\n",
    "# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\n",
    "#for i in range(temp, temp+50):\n",
    "for i in range(len(featureScrape)):\n",
    "    t = Thread(target=store_lyrics, args=(featureScrape['Song'][i],\n",
    "                featureScrape['Performer'][i], lyricsMap))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "scrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\n",
    "scrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\n",
    "\n",
    "# Get list of all improperly formatted songs and save to file\n",
    "problemSongs = []\n",
    "for k, v in lyricsMap.items():\n",
    "    if v[0][0] == \"*\":\n",
    "        problemSongs.append([k] + v[2:5])\n",
    "print(len(featureScrape), len(problemSongs))\n",
    "\n",
    "with open(\"data/problemSongs.txt\", \"w\") as file:\n",
    "    for s in problemSongs:\n",
    "        file.write(\"{}\\n\".format(s))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv of previously outputted scraped lyrics and reformat to match original\n",
    "# allLyrics = clean_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NLP pipeline to create tokens from lyrics\\nallLyrics[\\'Lyrics_tokenized\\'] = list(map(lyrics_tokenize, allLyrics[\\'Lyrics\\']))\\nallLyrics.dropna(inplace=True)\\nallLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# NLP pipeline to create tokens from lyrics\n",
    "allLyrics['Lyrics_tokenized'] = list(map(lyrics_tokenize, allLyrics['Lyrics']))\n",
    "allLyrics.dropna(inplace=True)\n",
    "allLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus and make dataframe with TF-IDF matrix\n",
    "allLyrics = pd.read_csv(\"data/lyricsTokenized.csv\")\n",
    "allLyrics.dropna(inplace=True)\n",
    "corpus = allLyrics['Lyrics_tokenized']\n",
    "tfidfLyrics = get_tfidf_matrix(corpus, 5000)\n",
    "tfidfLyrics.insert(0, \"SongID\", allLyrics['SongID'])\n",
    "tfidfLyrics.to_csv(\"data/tfidfMatrix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models with only lyrics, not counting other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with valence column from features to get valence of each song\n",
    "valenceOnly = pd.DataFrame({\"SongID\": features['SongID'], \"spotify_genre\": \n",
    "                            features['spotify_genre'], \"valence\": features['valence']})\n",
    "lyricsAndValence = tfidfLyrics.merge(valenceOnly, on='SongID')\n",
    "lyricsAndValence.set_index(\"SongID\", inplace=True)\n",
    "# Create new dataframe using classifier instead of regressor\n",
    "lyricsAndValenceBin = lyricsAndValence.copy()\n",
    "lyricsAndValenceBin['valence'] = (lyricsAndValenceBin['valence'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Run classifier models for pop genre\n",
    "lyricsAndValenceBinPop = lyricsAndValenceBin[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                                for g in lyricsAndValenceBin['spotify_genre']]]\n",
    "lyricsAndValenceBinPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceBinPop[lyricsAndValenceBinPop.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceBinPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5558646267964766, 0.39567430025445294, 0.3916876574307305)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.5559, 0.3957, 0.3917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time.time()\\nmodel.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\\nend = time.time()\\nprint(end-start)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3,                                         X_train, X_test, y_train, y_test)\\nprint(gradient_boost_class_results)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.6347, 0.0585, 0.4894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Run regressor models for pop genre\n",
    "lyricsAndValencePop = lyricsAndValence[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                            for g in lyricsAndValence['spotify_genre']]]\n",
    "lyricsAndValencePop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValencePop[lyricsAndValencePop.columns.difference(['valence'])]\n",
    "y = lyricsAndValencePop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\\ngbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\\nprint(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\\nscoreValencePop = gbr.score(X_test, y_test)\\ny_pred = gbr.predict(X_test)\\nrmseValencePop = np.sqrt(mean_squared_error(y_test, y_pred))\\nprint(scoreValencePop, rmseValencePop)\\ngradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3,                                         X_train, X_test, y_train, y_test)\\nprint(gradient_boost_class_results)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreValencePop = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseValencePop = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreValencePop, rmseValencePop)\n",
    "gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time.time()\\nmodel.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\\nend = time.time()\\nprint(end-start)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.004388753322895456, 0.23364992959108263)\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.006539, 0.2334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_profanity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-28e40d732acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatureNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyricsAndValencePop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feature_importance_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_profanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top Feature Importances of Pop (valence only)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filter_profanity' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "plots.make_feature_importance_plot(feature_importances, filter_profanity( \\\n",
    "                                   lyricsAndValencePop.columns[:-1]), 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Pop (valence only)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_valencepop.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(32, input_dim=X_train.shape[1]))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(32))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1))\n",
    "mlp.add(Activation(\"softmax\"))\n",
    "mlp.compile(optimizer=\"adadelta\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "mlp.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "score = mlp.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "# 0.2244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for rock/metal genres\n",
    "lyricsAndValenceBinRock = lyricsAndValenceBin[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                                for g in lyricsAndValenceBin['spotify_genre']]]\n",
    "lyricsAndValenceBinRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceBinRock[lyricsAndValenceBinRock.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceBinRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.5877, 0.4042, 0.3867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.6644, 0.0436, 0.4630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for rock/metal genres\n",
    "lyricsAndValenceRock = lyricsAndValence[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                            for g in lyricsAndValence['spotify_genre']]]\n",
    "lyricsAndValenceRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndValenceRock[lyricsAndValenceRock.columns.difference(['valence'])]\n",
    "y = lyricsAndValenceRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreValenceRock = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseValenceRock = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreValenceRock, rmseValenceRock)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.01272, 0.2339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "plots.make_feature_importance_plot(feature_importances, filter_profanity( \\\n",
    "                                   lyricsAndValenceRock.columns[:-1]), 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Rock (valence only)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_valenceRock.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(32, input_dim=X_train.shape[1]))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(32))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1))\n",
    "mlp.add(Activation(\"softmax\"))\n",
    "mlp.compile(optimizer=\"adadelta\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "mlp.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "score = mlp.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "# 0.2082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lyricsAndValence, lyricsAndValencePop, lyricsAndValenceRock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try adding all other numerical features to see if it improves accuracy/RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with features to get all numerical features as well as valence\n",
    "lyricsAndFeatures = tfidfLyrics.merge(features, on='SongID')\n",
    "lyricsAndFeatures.drop([\"Performer\", \"Song\"], axis=1, inplace=True)\n",
    "lyricsAndFeatures.set_index(\"SongID\", inplace=True)\n",
    "# Create new dataframe using classifier instead of regressor\n",
    "lyricsAndFeaturesBin = lyricsAndFeatures.copy()\n",
    "lyricsAndFeaturesBin['valence'] = (lyricsAndFeaturesBin['valence'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for pop genre\n",
    "lyricsAndFeaturesBinPop = lyricsAndFeaturesBin[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                                for g in lyricsAndFeaturesBin['spotify_genre']]]\n",
    "lyricsAndFeaturesBinPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesBinPop[lyricsAndFeaturesBinPop.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesBinPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.6796, 0.5280, 0.5646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_dgradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.7821, 0.5954, 0.7548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for pop genre\n",
    "lyricsAndFeaturesPop = lyricsAndFeatures[[contains_genre_type(g, [\"pop\"]) \\\n",
    "                            for g in lyricsAndFeatures['spotify_genre']]]\n",
    "lyricsAndFeaturesPop.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesPop[lyricsAndFeaturesPop.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesPop['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreFeaturesPop = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseFeaturesPop = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreFeaturesPop, rmseFeaturesPop)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"pop\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.4659, 0.1711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "plots.make_feature_importance_plot(feature_importances, filter_profanity( \\\n",
    "                                   lyricsAndFeaturesPop.columns[:-1]), 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Pop (all features)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_featurespop.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(32, input_dim=X_train.shape[1]))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(32))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1))\n",
    "mlp.add(Activation(\"softmax\"))\n",
    "mlp.compile(optimizer=\"adadelta\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "mlp.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "score = mlp.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "# 0.2244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier models for rock/metal genres\n",
    "lyricsAndFeaturesBinRock = lyricsAndFeaturesBin[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                                for g in lyricsAndFeaturesBin['spotify_genre']]]\n",
    "lyricsAndFeaturesBinRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesBinRock[lyricsAndFeaturesBinRock.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesBinRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logistic_regression_results = model.get_logistic_regression_results(X_train, \\\n",
    "                                        X_test, y_train, y_test)\n",
    "print(logistic_regression_results)\n",
    "# 0.7294, 0.6028, 0.5925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting classifier hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_class_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier model\n",
    "'''gradient_boost_class_results = model.get_gradient_boost_class_results(0.1, 140, 3, \\\n",
    "                                        X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_class_results)'''\n",
    "# 0.8055, 0.6045, 0.7626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressor models for rock/metal genres\n",
    "lyricsAndFeaturesRock = lyricsAndFeatures[[contains_genre_type(g, [\"rock\", \"metal\"]) \\\n",
    "                            for g in lyricsAndFeatures['spotify_genre']]]\n",
    "lyricsAndFeaturesRock.drop([\"spotify_genre\"], axis=1, inplace=True)\n",
    "X = lyricsAndFeaturesRock[lyricsAndFeaturesRock.columns.difference(['valence'])]\n",
    "y = lyricsAndFeaturesRock['valence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Grid search gradient boosting regressor hyperparameters and return model score and RMSE\n",
    "gbr = model.grid_search_gradient_boost(X_train, X_test, y_train, y_test)\n",
    "print(gbr.best_params_, np.sqrt(np.abs(gbr.best_score_)))\n",
    "scoreFeaturesRock = gbr.score(X_test, y_test)\n",
    "y_pred = gbr.predict(X_test)\n",
    "rmseFeaturesRock = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(scoreFeaturesRock, rmseFeaturesRock)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore gradient boosting regressor hyperparameters\n",
    "'''start = time.time()\n",
    "model.plot_gradient_boost_reg_hyperparameters(X_train, X_test, y_train, y_test, \"rock\")\n",
    "end = time.time()\n",
    "print(end-start)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor model\n",
    "gradient_boost_reg_results, feature_importances = model.get_gradient_boost_reg_results( \\\n",
    "                            0.05, 120, 3, X_train, X_test, y_train, y_test)\n",
    "print(gradient_boost_reg_results)\n",
    "# 0.5324, 0.1610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "plots.make_feature_importance_plot(feature_importances, filter_profanity( \\\n",
    "                                   lyricsAndFeaturesRock.columns[:-1]), 30, ax)\n",
    "fig.suptitle(\"Top Feature Importances of Rock (all features)\", fontsize=20)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig(\"images/featureImportances_featuresrock.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(32, input_dim=X_train.shape[1]))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(32))\n",
    "mlp.add(Activation(\"tanh\"))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1))\n",
    "mlp.add(Activation(\"softmax\"))\n",
    "mlp.compile(optimizer=\"adadelta\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "mlp.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "score = mlp.evaluate(X_test, y_test)\n",
    "print(score)\n",
    "# 0.2082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
