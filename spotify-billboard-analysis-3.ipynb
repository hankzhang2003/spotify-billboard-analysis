{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# git config --global credential.helper store\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import ssl\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                            GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"stopwords\", \"punkt\", \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"])\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "from clean_dfs import clean_features, clean_weeks, clean_lyrics\n",
    "from web_scraping import parse_page, store_lyrics\n",
    "from nlp_pipeline import lyrics_tokenize, get_tfidf_matrix\n",
    "from genre_helper_functions import get_bucket, contains_genre_type, create_genre_column\n",
    "from make_plots import (make_frequency_plot, make_line_plot, make_dual_plot_same,\n",
    "                        make_dual_plot_mixed, make_scatter)\n",
    "import modeling_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_features()\n",
    "#weeks = clean_weeks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joined = weeks.merge(features, on=\\'SongID\\')\\n#joined.to_csv(\"data/joined.csv\", index=False)\\n\\n# Expand genres into individual components\\nfeatureGenres = features.explode(\\'spotify_genre\\')\\nfeatureGenres = featureGenres[featureGenres[\\'spotify_genre\\'] != \\'\\']\\n\\njoinedGenres = joined.explode(\\'spotify_genre\\')\\njoinedGenres = joinedGenres[joinedGenres[\\'spotify_genre\\'] != \\'\\']\\n\\nexplicitness = joined[[\\'Year\\', \\'spotify_track_explicit\\']]\\nexplicitness = explicitness.groupby([\\'Year\\']).mean().reset_index()\\n\\nnumericalMetrics = joined.columns.tolist()[11:23]\\nnumericals = joined[[\\'Year\\'] + numericalMetrics].groupby([\\'Year\\']).mean().reset_index()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''joined = weeks.merge(features, on='SongID')\n",
    "#joined.to_csv(\"data/joined.csv\", index=False)\n",
    "\n",
    "# Expand genres into individual components\n",
    "featureGenres = features.explode('spotify_genre')\n",
    "featureGenres = featureGenres[featureGenres['spotify_genre'] != '']\n",
    "\n",
    "joinedGenres = joined.explode('spotify_genre')\n",
    "joinedGenres = joinedGenres[joinedGenres['spotify_genre'] != '']\n",
    "\n",
    "explicitness = joined[['Year', 'spotify_track_explicit']]\n",
    "explicitness = explicitness.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "numericalMetrics = joined.columns.tolist()[11:23]\n",
    "numericals = joined[['Year'] + numericalMetrics].groupby(['Year']).mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Normalize numerical features not between 0 and 1\\nfeatureGenresNorm = featureGenres.copy()\\nscaled = [\"track_duration\", \"loudness\", \"tempo\"]\\nfor metric in scaled:\\n    mms = MinMaxScaler()\\n    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm[\\'track_duration\\'].                                 to_numpy().reshape(-1, 1))\\n\\n# Create grouped tables\\ngenres = featureGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoined = joinedGenres.groupby([\\'spotify_genre\\'])[\\'SongID\\'].count().reset_index()\\ngenresJoinedDecade = joinedGenres.groupby([\\'spotify_genre\\', \\'Decade\\'])[\\'SongID\\'].count().                         reset_index().sort_values(by=\"Decade\")\\ngenreFeatures = featureGenresNorm.groupby([\\'spotify_genre\\'])[numericalMetrics].mean().reset_index()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Normalize numerical features not between 0 and 1\n",
    "featureGenresNorm = featureGenres.copy()\n",
    "scaled = [\"track_duration\", \"loudness\", \"tempo\"]\n",
    "for metric in scaled:\n",
    "    mms = MinMaxScaler()\n",
    "    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm['track_duration']. \\\n",
    "                                to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Create grouped tables\n",
    "genres = featureGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoined = joinedGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoinedDecade = joinedGenres.groupby(['spotify_genre', 'Decade'])['SongID'].count(). \\\n",
    "                        reset_index().sort_values(by=\"Decade\")\n",
    "genreFeatures = featureGenresNorm.groupby(['spotify_genre'])[numericalMetrics].mean().reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Web scrape lyrics\\nfeatureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre                              in features[\\'spotify_genre\\']]].reset_index(drop=True)\\nlyricsMap = {}\\nthreads = []\\ntemp = 0\\nstart = time.time()\\n# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\\n#for i in range(temp, temp+50):\\nfor i in range(len(featureScrape)):\\n    t = Thread(target=store_lyrics, args=(featureScrape[\\'Song\\'][i],\\n                featureScrape[\\'Performer\\'][i], lyricsMap))\\n    threads.append(t)\\n    t.start()\\nfor t in threads:\\n    t.join()\\nend = time.time()\\nprint(end - start)\\nscrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\\nscrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\\n\\n# Get list of all improperly formatted songs and save to file\\nproblemSongs = []\\nfor k, v in lyricsMap.items():\\n    if v[0][0] == \"*\":\\n        problemSongs.append([k] + v[2:5])\\nprint(len(featureScrape), len(problemSongs))\\n\\nwith open(\"data/problemSongs.txt\", \"w\") as file:\\n    for s in problemSongs:\\n        file.write(\"{}\\n\".format(s))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Web scrape lyrics\n",
    "featureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre \\\n",
    "                             in features['spotify_genre']]].reset_index(drop=True)\n",
    "lyricsMap = {}\n",
    "threads = []\n",
    "temp = 0\n",
    "start = time.time()\n",
    "# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\n",
    "#for i in range(temp, temp+50):\n",
    "for i in range(len(featureScrape)):\n",
    "    t = Thread(target=store_lyrics, args=(featureScrape['Song'][i],\n",
    "                featureScrape['Performer'][i], lyricsMap))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "scrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\n",
    "scrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)\n",
    "\n",
    "# Get list of all improperly formatted songs and save to file\n",
    "problemSongs = []\n",
    "for k, v in lyricsMap.items():\n",
    "    if v[0][0] == \"*\":\n",
    "        problemSongs.append([k] + v[2:5])\n",
    "print(len(featureScrape), len(problemSongs))\n",
    "\n",
    "with open(\"data/problemSongs.txt\", \"w\") as file:\n",
    "    for s in problemSongs:\n",
    "        file.write(\"{}\\n\".format(s))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv of previously outputted scraped lyrics and reformat to match original\n",
    "allLyrics = clean_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# NLP pipeline to create tokens from lyrics\\nallLyrics[\\'Lyrics_tokenized\\'] = list(map(lyrics_tokenize, allLyrics[\\'Lyrics\\']))\\nallLyrics.dropna(inplace=True)\\nallLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# NLP pipeline to create tokens from lyrics\n",
    "allLyrics['Lyrics_tokenized'] = list(map(lyrics_tokenize, allLyrics['Lyrics']))\n",
    "allLyrics.dropna(inplace=True)\n",
    "allLyrics.to_csv(\"data/lyricsTokenized.csv\", index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus and make dataframe with TF-IDF matrix\n",
    "allLyrics = pd.read_csv(\"data/lyricsTokenized.csv\")\n",
    "allLyrics.dropna(inplace=True)\n",
    "corpus = allLyrics['Lyrics_tokenized']\n",
    "tfidfLyrics = get_tfidf_matrix(corpus)\n",
    "tfidfLyrics.insert(0, \"SongID\", allLyrics['SongID'])\n",
    "tfidfLyrics.to_csv(\"data/tfidfMatrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfLyrics = pd.read_csv(\"data/tfidfMatrix.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.72286880679317"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "33\n",
      "35\n",
      "41\n",
      "58\n",
      "83\n",
      "91\n",
      "92\n",
      "106\n",
      "112\n",
      "140\n",
      "162\n",
      "186\n",
      "192\n",
      "193\n",
      "195\n",
      "198\n",
      "234\n",
      "235\n",
      "237\n",
      "252\n",
      "262\n",
      "269\n",
      "290\n",
      "296\n",
      "319\n",
      "335\n",
      "339\n",
      "345\n",
      "355\n",
      "397\n",
      "458\n",
      "460\n",
      "477\n",
      "486\n",
      "496\n",
      "565\n",
      "574\n",
      "618\n",
      "643\n",
      "644\n",
      "651\n",
      "675\n",
      "697\n",
      "710\n",
      "740\n",
      "747\n",
      "815\n",
      "834\n",
      "863\n",
      "924\n",
      "975\n",
      "987\n",
      "995\n",
      "1007\n",
      "1017\n",
      "1098\n",
      "1106\n",
      "1113\n",
      "1114\n",
      "1116\n",
      "1145\n",
      "1150\n",
      "1269\n",
      "1275\n",
      "1278\n",
      "1350\n",
      "1354\n",
      "1377\n",
      "1396\n",
      "1415\n",
      "1419\n",
      "1430\n",
      "1436\n",
      "1440\n",
      "1446\n",
      "1455\n",
      "1459\n",
      "1481\n",
      "1502\n",
      "1535\n",
      "1564\n",
      "1605\n",
      "1668\n",
      "1712\n",
      "1719\n",
      "1729\n",
      "1770\n",
      "1807\n",
      "1814\n",
      "1818\n",
      "1833\n",
      "1839\n",
      "1860\n",
      "1868\n",
      "1876\n",
      "1896\n",
      "1898\n",
      "1907\n",
      "1922\n",
      "1953\n",
      "1961\n",
      "1983\n",
      "1992\n",
      "1994\n",
      "2018\n",
      "2050\n",
      "2062\n",
      "2072\n",
      "2150\n",
      "2153\n",
      "2160\n",
      "2190\n",
      "2193\n",
      "2196\n",
      "2258\n",
      "2304\n",
      "2347\n",
      "2350\n",
      "2378\n",
      "2406\n",
      "2415\n",
      "2416\n",
      "2439\n",
      "2441\n",
      "2457\n",
      "2468\n",
      "2490\n",
      "2507\n",
      "2515\n",
      "2539\n",
      "2553\n",
      "2556\n",
      "2567\n",
      "2568\n",
      "2587\n",
      "2596\n",
      "2627\n",
      "2628\n",
      "2633\n",
      "2712\n",
      "2737\n",
      "2752\n",
      "2767\n",
      "2776\n",
      "2791\n",
      "2801\n",
      "2874\n",
      "2939\n",
      "2967\n",
      "2973\n",
      "3025\n",
      "3028\n",
      "3058\n",
      "3066\n",
      "3068\n",
      "3098\n",
      "3104\n",
      "3113\n",
      "3122\n",
      "3148\n",
      "3149\n",
      "3153\n",
      "3199\n",
      "3228\n",
      "3263\n",
      "3284\n",
      "3306\n",
      "3342\n",
      "3352\n",
      "3353\n",
      "3355\n",
      "3362\n",
      "3380\n",
      "3406\n",
      "3453\n",
      "3511\n",
      "3610\n",
      "3620\n",
      "3687\n",
      "3688\n",
      "3726\n",
      "3731\n",
      "3735\n",
      "3803\n",
      "3811\n",
      "3817\n",
      "3840\n",
      "3853\n",
      "3861\n",
      "3889\n",
      "3936\n",
      "3963\n",
      "3979\n",
      "3985\n",
      "3989\n",
      "4019\n",
      "4027\n",
      "4030\n",
      "4047\n",
      "4055\n",
      "4062\n",
      "4070\n",
      "4105\n",
      "4110\n",
      "4117\n",
      "4120\n",
      "4133\n",
      "4157\n",
      "4163\n",
      "4195\n",
      "4208\n",
      "4252\n",
      "4284\n",
      "4414\n",
      "4423\n",
      "4431\n",
      "4443\n",
      "4446\n",
      "4506\n",
      "4539\n",
      "4541\n",
      "4571\n",
      "4601\n",
      "4625\n",
      "4688\n",
      "4694\n",
      "4716\n",
      "4727\n",
      "4756\n",
      "4785\n",
      "4860\n",
      "4901\n",
      "4925\n",
      "4940\n",
      "4974\n",
      "4979\n",
      "5002\n",
      "5014\n",
      "5081\n",
      "5097\n",
      "5137\n",
      "5163\n",
      "5195\n",
      "5227\n",
      "5233\n",
      "5234\n",
      "5238\n",
      "5245\n",
      "5260\n",
      "5291\n",
      "5297\n",
      "5299\n",
      "5308\n",
      "5326\n",
      "5334\n",
      "5336\n",
      "5340\n",
      "5342\n",
      "5420\n",
      "5427\n",
      "5480\n",
      "5484\n",
      "5489\n",
      "5503\n",
      "5511\n",
      "5515\n",
      "5540\n",
      "5551\n",
      "5570\n",
      "5612\n",
      "5648\n",
      "5770\n",
      "5792\n",
      "5822\n",
      "5898\n",
      "5899\n",
      "5930\n",
      "5935\n",
      "5950\n",
      "5990\n",
      "5993\n",
      "5996\n",
      "6003\n",
      "6006\n",
      "6011\n",
      "6014\n",
      "6020\n",
      "6096\n",
      "6121\n",
      "6124\n",
      "6171\n",
      "6190\n",
      "6248\n",
      "6279\n",
      "6292\n",
      "6316\n",
      "6351\n",
      "6367\n",
      "6422\n",
      "6431\n",
      "6434\n",
      "6442\n",
      "6443\n",
      "6478\n",
      "6482\n",
      "6486\n",
      "6487\n",
      "6490\n",
      "6523\n",
      "6572\n",
      "6588\n",
      "6596\n",
      "6612\n",
      "6664\n",
      "6675\n",
      "6732\n",
      "6748\n",
      "6781\n",
      "6846\n",
      "6855\n",
      "6897\n",
      "6908\n",
      "6936\n",
      "7052\n",
      "7146\n",
      "7178\n",
      "7231\n",
      "7252\n",
      "7253\n",
      "7256\n",
      "7306\n",
      "7315\n",
      "7340\n",
      "7355\n",
      "7409\n",
      "7506\n",
      "7535\n",
      "7536\n",
      "7545\n",
      "7561\n",
      "7574\n",
      "7588\n",
      "7618\n",
      "7653\n",
      "7660\n",
      "7672\n",
      "7707\n",
      "7727\n",
      "7734\n",
      "7740\n",
      "7755\n",
      "7760\n",
      "7761\n",
      "7789\n",
      "7801\n",
      "7846\n",
      "7851\n",
      "7869\n",
      "7934\n",
      "7972\n",
      "7974\n",
      "7978\n",
      "7981\n",
      "8003\n",
      "8018\n",
      "8022\n",
      "8054\n",
      "8058\n",
      "8094\n",
      "8101\n",
      "8183\n",
      "8201\n",
      "8208\n",
      "8211\n",
      "8268\n",
      "8302\n",
      "8304\n",
      "8372\n",
      "8411\n",
      "8436\n",
      "8444\n",
      "8449\n",
      "8496\n",
      "8537\n",
      "8567\n",
      "8624\n",
      "8650\n",
      "8657\n",
      "8710\n",
      "8747\n",
      "8785\n",
      "8787\n",
      "8820\n",
      "8881\n",
      "8938\n",
      "8942\n",
      "9012\n",
      "9016\n",
      "9034\n",
      "9039\n",
      "9057\n",
      "9075\n",
      "9086\n",
      "9089\n",
      "9090\n",
      "9093\n",
      "9135\n",
      "9138\n",
      "9143\n",
      "9159\n",
      "9168\n",
      "9194\n",
      "9214\n",
      "9219\n",
      "9325\n",
      "9326\n",
      "9380\n",
      "9383\n",
      "9386\n",
      "9392\n",
      "9439\n",
      "9453\n",
      "9502\n",
      "9508\n",
      "9554\n",
      "9555\n",
      "9556\n",
      "9602\n",
      "9648\n",
      "9654\n",
      "9673\n",
      "9682\n",
      "9726\n",
      "9734\n",
      "9790\n",
      "9834\n",
      "9893\n",
      "9910\n",
      "9935\n",
      "9969\n",
      "9977\n",
      "10009\n",
      "10012\n",
      "10015\n",
      "10017\n",
      "10018\n",
      "10036\n",
      "10051\n",
      "10058\n",
      "10097\n",
      "10120\n",
      "10155\n",
      "10164\n",
      "10167\n",
      "10169\n",
      "10175\n",
      "10217\n",
      "10296\n",
      "10356\n",
      "10358\n",
      "10362\n",
      "10400\n",
      "10434\n",
      "10445\n",
      "10448\n",
      "10451\n",
      "10459\n",
      "10534\n",
      "10648\n",
      "10687\n",
      "10692\n",
      "10699\n",
      "10770\n",
      "10776\n",
      "10780\n",
      "10801\n",
      "10808\n",
      "10827\n",
      "10831\n",
      "10875\n",
      "10879\n",
      "10931\n",
      "10936\n",
      "11013\n",
      "11020\n",
      "11030\n",
      "11031\n",
      "11068\n",
      "11117\n",
      "11124\n",
      "11127\n",
      "11130\n",
      "11132\n",
      "11144\n",
      "11197\n",
      "11200\n",
      "11207\n",
      "11226\n",
      "11269\n",
      "11287\n",
      "11332\n",
      "11337\n",
      "11339\n",
      "11343\n",
      "11358\n",
      "11363\n",
      "11437\n",
      "11441\n",
      "11452\n",
      "11461\n",
      "11523\n",
      "11524\n",
      "11573\n",
      "11582\n",
      "11595\n",
      "11633\n",
      "11668\n",
      "11698\n",
      "11703\n",
      "11706\n",
      "11735\n",
      "11746\n",
      "11773\n",
      "11805\n",
      "11820\n",
      "11828\n",
      "11830\n",
      "11854\n",
      "11858\n",
      "11908\n",
      "11961\n",
      "11963\n",
      "12003\n",
      "12012\n",
      "12015\n",
      "12016\n",
      "12019\n",
      "12042\n",
      "12054\n",
      "12065\n",
      "12119\n",
      "12145\n",
      "12174\n",
      "12214\n",
      "12223\n",
      "12227\n",
      "12234\n",
      "12239\n",
      "12240\n",
      "12254\n",
      "12258\n",
      "12304\n",
      "12326\n",
      "12340\n",
      "12459\n",
      "12473\n",
      "12484\n",
      "12493\n",
      "12517\n",
      "12531\n",
      "12571\n",
      "12574\n",
      "12576\n",
      "12609\n",
      "12613\n",
      "12635\n",
      "12638\n",
      "12641\n",
      "12748\n",
      "12795\n",
      "12851\n",
      "12896\n",
      "12909\n",
      "12974\n",
      "12995\n",
      "13036\n",
      "13044\n",
      "13055\n",
      "13059\n",
      "13078\n",
      "13107\n",
      "13174\n",
      "13193\n",
      "13199\n",
      "13213\n",
      "13309\n",
      "13314\n",
      "13317\n",
      "13327\n",
      "13331\n",
      "13344\n",
      "13391\n",
      "13394\n",
      "13398\n",
      "13410\n",
      "13430\n",
      "13438\n",
      "13463\n",
      "13506\n",
      "13519\n",
      "13532\n",
      "13550\n",
      "13561\n",
      "13589\n",
      "13610\n",
      "13616\n",
      "13618\n",
      "13619\n",
      "13627\n",
      "13643\n",
      "13648\n",
      "13662\n",
      "13677\n",
      "13683\n",
      "13711\n",
      "13740\n",
      "13782\n",
      "13805\n",
      "13812\n",
      "13837\n",
      "13846\n",
      "13891\n",
      "13939\n",
      "13951\n",
      "14106\n",
      "14125\n",
      "14127\n",
      "14128\n",
      "14197\n",
      "14225\n",
      "14234\n",
      "14257\n",
      "14331\n",
      "14371\n",
      "14413\n",
      "14486\n",
      "14490\n",
      "14500\n",
      "14513\n",
      "14537\n",
      "14549\n",
      "14562\n",
      "14599\n",
      "14606\n",
      "14630\n",
      "14644\n",
      "14658\n",
      "14682\n",
      "14697\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
