{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# git config --global credential.helper store\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import ssl\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from threading import Thread\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier)\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"stopwords\", \"punkt\", \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"])\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "from clean_dfs import clean_features, clean_weeks, clean_lyrics\n",
    "from web_scraping import parse_page, store_lyrics\n",
    "from nlp_pipeline import lyrics_tokenize\n",
    "from genre_helper_functions import get_bucket, contains_genre_type, create_genre_column\n",
    "from make_plots import (make_frequency_plot, make_line_plot, make_dual_plot_same,\n",
    "                        make_dual_plot_mixed, make_scatter)\n",
    "import modeling_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_features()\n",
    "weeks = clean_weeks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "joined = weeks.merge(features, on='SongID')\n",
    "#joined.to_csv(\"data/joined.csv\", index=False)\n",
    "\n",
    "# Expand genres into individual components\n",
    "featureGenres = features.explode('spotify_genre')\n",
    "featureGenres = featureGenres[featureGenres['spotify_genre'] != '']\n",
    "\n",
    "joinedGenres = joined.explode('spotify_genre')\n",
    "joinedGenres = joinedGenres[joinedGenres['spotify_genre'] != '']\n",
    "\n",
    "explicitness = joined[['Year', 'spotify_track_explicit']]\n",
    "explicitness = explicitness.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "numericalMetrics = joined.columns.tolist()[11:23]\n",
    "numericals = joined[['Year'] + numericalMetrics].groupby(['Year']).mean().reset_index()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Normalize numerical features not between 0 and 1\n",
    "featureGenresNorm = featureGenres.copy()\n",
    "scaled = [\"track_duration\", \"loudness\", \"tempo\"]\n",
    "for metric in scaled:\n",
    "    mms = MinMaxScaler()\n",
    "    featureGenresNorm[metric] = mms.fit_transform(featureGenresNorm['track_duration']. \\\n",
    "                                to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Create grouped tables\n",
    "genres = featureGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoined = joinedGenres.groupby(['spotify_genre'])['SongID'].count().reset_index()\n",
    "genresJoinedDecade = joinedGenres.groupby(['spotify_genre', 'Decade'])['SongID'].count(). \\\n",
    "                        reset_index().sort_values(by=\"Decade\")\n",
    "genreFeatures = featureGenresNorm.groupby(['spotify_genre'])[numericalMetrics].mean().reset_index()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpage = parse_page(\"Dance the Night Away\", \"Twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scrape lyrics\n",
    "featureScrape = features.loc[[contains_genre_type(genre, [\"pop\", \"rock\", \"metal\"]) for genre \\\n",
    "                             in features['spotify_genre']]].reset_index(drop=True)\n",
    "lyricsMap = {}\n",
    "threads = []\n",
    "temp = 0\n",
    "start = time.time()\n",
    "# Write scraped lyrics to hashmap, parallelize to save time (thread safe because no unique keys)\n",
    "#for i in range(temp, temp+50):\n",
    "for i in range(len(featureScrape)):\n",
    "    t = Thread(target=store_lyrics, args=(featureScrape['Song'][i],\n",
    "                featureScrape['Performer'][i], lyricsMap))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "scrapedLyrics = pd.DataFrame(lyricsMap.items(), columns=[\"SongID\", \"Lyrics\"])\n",
    "scrapedLyrics.to_csv(\"data/scrapedLyrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all improperly formatted songs and save to file\n",
    "problemSongs = []\n",
    "for k, v in lyricsMap.items():\n",
    "    if v[0][0] == \"*\":\n",
    "        problemSongs.append([k] + v[2:5])\n",
    "print(len(featureScrape), len(problemSongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/problemSongs.txt\", \"w\") as file:\n",
    "    for s in problemSongs:\n",
    "        file.write(\"{}\\n\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanana nanana You and me in the moonlight A night of stars flowers and festivals The sound of the waves is on and we’re dancing right now This feels so perfect Hey ocean come play with us Hey wind come over here Under the moonlight it’s our world Everyone together party all night long yeah it’s good If you wanna have some fun Don’t let go of this special happiness That’s like the salty air One two three let’s go Let’s go dance like flying to space Hey! Let’s dance the night away Let’s dance the night away One two three let’s go Scream so loud they can hear you across the ocean Let’s dance the night away Dance the night away Let’s dance the night away Dance the night away Let’s dance the night away You and me in this cool night The half-moon is smiling Let’s promise to go behind the moon some day And throw a party yeah it’s good If you wanna have some fun Don’t let go of this special happiness That’s like the silver sand One two three let’s go Let’s go dance like flying to space Hey! Let’s dance the night away Let’s dance the night away One two three let’s go Scream so loud they can hear you across the ocean Let’s dance the night away Dance the night away Let’s dance the night away Dance the night away Let’s dance the night away Shout as if today is the last far far away Jump as if you’re flying higher and higher Shout as if today is the last far far away With the spilling starlight let’s dance the night away Let’s dance the night away One two three let’s go Scream so loud they can hear you across the ocean Let’s dance the night away Dance the night away Let’s dance the night away Dance the night away Let’s dance the night away\n",
      "b'nanana nanana a night flower sound feel hey hey moonlight world everyon parti night good wan dont special happi that salti air let let danc space hey let night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night night half-moon let promis moon day parti good wan dont special happi that silver let let danc space hey let night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night shout today last jump your higher higher shout today last starlight let danc night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night\n"
     ]
    }
   ],
   "source": [
    "testpage = parse_page(\"Dance the Night Away\", \"Twice\")\n",
    "testpage = [line.replace(\",\", \"\") for line in testpage]\n",
    "testpage = clean_lyrics(testpage)\n",
    "print(testpage)\n",
    "testpage_tokenized = lyrics_tokenize(testpage)\n",
    "print(testpage_tokenized)\n",
    "testpage2 = parse_page(\"Fake Love\", \"BTS\")\n",
    "testpage2 = [line.replace(\",\", \"\") for line in testpage2]\n",
    "testpage2 = clean_lyrics(testpage2)\n",
    "testpage2_tokenized = lyrics_tokenize(testpage2)\n",
    "print(testpage2_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'nanana nanana a night flower sound feel hey hey moonlight world everyon parti night good wan dont special happi that salti air let let danc space hey let night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night night half-moon let promis moon day parti good wan dont special happi that silver let let danc space hey let night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night shout today last jump your higher higher shout today last starlight let danc night let danc night let scream loud hear ocean let danc night danc night let danc night danc night let danc night\", 'b strong wish love perfect love weak cant dream cant true sick fake fake fake love sorri fake love fake fake love good man world everyth forest rout quit unsur mirror hell strong wish love perfect love weak cant dream cant true love bad love bad mold pretti lie love mad love mad tri eras doll love bad love bad mold pretti lie love mad love mad tri eras doll sick fake fake fake love sorri fake love fake fake sad know smile look cant understand im im mean im blind love heck love fake love woo woo know woo cuz fake fake love love bad love bad mold pretti lie love mad love mad tri eras doll love bad love bad mold pretti lie love mad love mad tri eras doll sick fake fake fake love sorri fake love fake fake love strong wish love perfect love weak cant dream cant true']\n",
      "{'nanana': 41, 'night': 42, 'flower': 15, 'sound': 58, 'feel': 14, 'hey': 23, 'moonlight': 40, 'world': 73, 'everyon': 11, 'parti': 44, 'good': 17, 'wan': 69, 'dont': 8, 'special': 60, 'happi': 19, 'that': 63, 'salti': 51, 'air': 0, 'let': 29, 'danc': 5, 'space': 59, 'scream': 52, 'loud': 32, 'hear': 20, 'ocean': 43, 'half': 18, 'moon': 39, 'promis': 47, 'day': 6, 'silver': 55, 'shout': 53, 'today': 64, 'last': 28, 'jump': 26, 'your': 74, 'higher': 24, 'starlight': 61, 'strong': 62, 'wish': 71, 'love': 33, 'perfect': 45, 'weak': 70, 'cant': 3, 'dream': 9, 'true': 66, 'sick': 54, 'fake': 13, 'sorri': 57, 'man': 35, 'everyth': 12, 'forest': 16, 'rout': 49, 'quit': 48, 'unsur': 68, 'mirror': 37, 'hell': 22, 'bad': 1, 'mold': 38, 'pretti': 46, 'lie': 30, 'mad': 34, 'tri': 65, 'eras': 10, 'doll': 7, 'sad': 50, 'know': 27, 'smile': 56, 'look': 31, 'understand': 67, 'im': 25, 'mean': 36, 'blind': 2, 'heck': 21, 'woo': 72, 'cuz': 4}\n",
      "[[0.02449589 0.         0.         0.         0.         0.51441375\n",
      "  0.02449589 0.         0.04899179 0.         0.         0.02449589\n",
      "  0.         0.         0.02449589 0.02449589 0.         0.03485806\n",
      "  0.02449589 0.04899179 0.07348768 0.         0.         0.09798357\n",
      "  0.04899179 0.         0.02449589 0.         0.04899179 0.56340553\n",
      "  0.         0.         0.07348768 0.         0.         0.\n",
      "  0.         0.         0.         0.04899179 0.02449589 0.04899179\n",
      "  0.58790143 0.07348768 0.04899179 0.         0.         0.02449589\n",
      "  0.         0.         0.         0.02449589 0.07348768 0.04899179\n",
      "  0.         0.02449589 0.         0.         0.02449589 0.04899179\n",
      "  0.04899179 0.02449589 0.         0.04899179 0.04899179 0.\n",
      "  0.         0.         0.         0.04899179 0.         0.\n",
      "  0.         0.01742903 0.02449589]\n",
      " [0.         0.17974068 0.02246758 0.15727309 0.02246758 0.\n",
      "  0.         0.08987034 0.         0.06740275 0.08987034 0.\n",
      "  0.02246758 0.47181928 0.         0.         0.02246758 0.01598587\n",
      "  0.         0.         0.         0.02246758 0.02246758 0.\n",
      "  0.         0.06740275 0.         0.04493517 0.         0.\n",
      "  0.08987034 0.02246758 0.         0.76389789 0.17974068 0.02246758\n",
      "  0.02246758 0.02246758 0.08987034 0.         0.         0.\n",
      "  0.         0.         0.         0.06740275 0.08987034 0.\n",
      "  0.02246758 0.02246758 0.02246758 0.         0.         0.\n",
      "  0.06740275 0.         0.02246758 0.06740275 0.         0.\n",
      "  0.         0.         0.06740275 0.         0.         0.08987034\n",
      "  0.06740275 0.02246758 0.02246758 0.         0.06740275 0.06740275\n",
      "  0.06740275 0.01598587 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "testcorpus = [testpage_tokenized, testpage2_tokenized]\n",
    "print(testcorpus)\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfMatrix = tfidf.fit_transform(testcorpus)\n",
    "print(tfidf.vocabulary_)\n",
    "print(tfidfMatrix.todense()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv of previously outputted scraped lyrics and reformat to match original\n",
    "allLyrics = clean_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "# NLP pipeline to create tokens from lyrics\n",
    "allLyrics['Lyrics_tokenized'] = list(map(lyrics_tokenize, allLyrics['Lyrics']))\n",
    "allLyrics.to_csv(\"data/allLyricsTokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus and make TF-IDF\n",
    "corpus = allLyrics['Lyrics_tokenized']\n",
    "tfidf = TfidfVectorizer(max_features=50000)\n",
    "tfidfMatrix = tfidf.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
